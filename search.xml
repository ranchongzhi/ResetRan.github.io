<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python爬虫（4）——XPath语法和lxml</title>
      <link href="2020/07/07/python-spyder-4/"/>
      <url>2020/07/07/python-spyder-4/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前面的python爬虫（1-3）讲了怎么向服务器发送请求，获得网页的源代码，不过一个网页的源代码有很多部分我们都是用不上的，那用什么方法能快速获取到我们想要的数据呢？这个时候，网页解析就派上用场了，常用的解析方法有：XPath语句、beautifulsoup4库、re正则表达式，这里先来介绍第一种。</p><h1 id="XPath"><a href="#XPath" class="headerlink" title="XPath"></a>XPath</h1><h2 id="什么是XPath"><a href="#什么是XPath" class="headerlink" title="什么是XPath"></a>什么是XPath</h2><p>xpath（XML Path Language）是一门在 XML 和 HTML 文档中查找信息的语言，可用来在 XML和 HTML 文档中对元素和属性进行遍历。简单点说，这玩意儿就是用来帮我们提取网页源代码中我们所需要的数据的，比如标签属性值、文本值、链接等等都能通过XPath语法提取。</p><h2 id="XPath语法的开发工具（Chrome插件）"><a href="#XPath语法的开发工具（Chrome插件）" class="headerlink" title="XPath语法的开发工具（Chrome插件）"></a>XPath语法的开发工具（Chrome插件）</h2><p>我们想看XPath语法实时的效果，我觉得直接在浏览器里边写是最直观的方法了。谷歌浏览器的插件就帮我们解决了这个问题，插件名字叫“XPath Helper”，直接安装就行，安装步骤可以百度，教程一大把。<br>来看看使用，这里以豆瓣网为例子：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594126565193.jpg"><br>我先写一句做个例子（取电影汉密尔顿的豆瓣评分）：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594127192478.jpg"><br>匹配好之后，我们就可以把语句复制下来粘贴到代码中。</p><h2 id="XPath语法详解"><a href="#XPath语法详解" class="headerlink" title="XPath语法详解"></a>XPath语法详解</h2><p>关于XPath的语法介绍有很多，这里简单罗列：</p><h3 id="选取节点"><a href="#选取节点" class="headerlink" title="选取节点"></a>选取节点</h3><table><thead><tr><th align="center">表达式</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">nodename</td><td align="center">选取此节点的所有子节点</td></tr><tr><td align="center">/</td><td align="center">从根节点选取</td></tr><tr><td align="center">//</td><td align="center">从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置</td></tr><tr><td align="center">.</td><td align="center">选取当前节点</td></tr><tr><td align="center">..</td><td align="center">选取当前节点的父节点</td></tr><tr><td align="center">@</td><td align="center">选取属性</td></tr></tbody></table><h3 id="谓语"><a href="#谓语" class="headerlink" title="谓语"></a>谓语</h3><p>谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：<br>|路径表达式 |描述|<br>|:-:|:-:|<br>|/bookstore/book[1] |选取 bookstore 下的第一个子元素|<br>|/bookstore/book[last()] |选取 bookstore 下的倒数第二个 book 元素。|<br>|bookstore/book[position()&lt;<strong>3</strong>]| 选取 bookstore 下前面两个子元素。|<br>|//book[@price] |选取拥有 price 属性的 book 元素|<br>|//book[@price=10] |选取所有属性 price 等于 10 的 book 元素|<br>|//div[contains(@id,’1234’)] |选取所以属性 id 中含有 1234 的 div 元素|</p><h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><table><thead><tr><th align="center">通配符</th><th align="center">描述</th><th align="center">示例</th><th align="center">结果</th></tr></thead><tbody><tr><td align="center">*</td><td align="center">匹配任意节点</td><td align="center">/bookstore/*</td><td align="center">选取 bookstore 下的所有子元素。</td></tr><tr><td align="center">@*</td><td align="center">匹配节点中的任何属性</td><td align="center">//book[@*]</td><td align="center">选取所有带有属性的 book 元素。</td></tr></tbody></table><h3 id="选取多个路径"><a href="#选取多个路径" class="headerlink" title="选取多个路径"></a>选取多个路径</h3><p>通过在路径表达式中使用“|”运算符，可以选取若干个路径。示例如下：<br>**//bookstore/book | //book/title**（选取所有 book 元素以及 book 元素下所有的 title 元素）</p><h3 id="XPath运算符"><a href="#XPath运算符" class="headerlink" title="XPath运算符"></a>XPath运算符</h3><table><thead><tr><th align="center">运算符</th><th align="center">描述</th><th align="center">实例</th><th align="center">返回值</th></tr></thead><tbody><tr><td align="center">|</td><td align="center">计算两个节点集</td><td align="center">//book | //cd</td><td align="center">返回所有拥有 book 和 cd 元素的节点集</td></tr><tr><td align="center">+</td><td align="center">加法</td><td align="center">6 + 4</td><td align="center">10</td></tr><tr><td align="center">-</td><td align="center">减法</td><td align="center">6 - 4</td><td align="center">2</td></tr><tr><td align="center">*</td><td align="center">乘法</td><td align="center">6 * 4</td><td align="center">24</td></tr><tr><td align="center">div</td><td align="center">除法</td><td align="center">8 div 4</td><td align="center">2</td></tr><tr><td align="center">=</td><td align="center">等于</td><td align="center">price=9.80</td><td align="center">如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回false。</td></tr><tr><td align="center">!=</td><td align="center">不等于</td><td align="center">price!=9.80</td><td align="center">如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td></tr><tr><td align="center">&lt;</td><td align="center">小于</td><td align="center">price&lt;9.80</td><td align="center">如果price是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td></tr><tr><td align="center">&lt;=</td><td align="center">小于或等于</td><td align="center">price&lt;=9.80</td><td align="center">如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。</td></tr><tr><td align="center">&gt;</td><td align="center">大于</td><td align="center">price&gt;9.80</td><td align="center">如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。</td></tr><tr><td align="center">&gt;=</td><td align="center">大于或等于</td><td align="center">price&gt;=9.80</td><td align="center">如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。</td></tr><tr><td align="center">or</td><td align="center">或</td><td align="center">price=9.80 orprice=9.70</td><td align="center">如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。</td></tr><tr><td align="center">and</td><td align="center">与</td><td align="center">price&gt;9.00 andprice&lt;9.90</td><td align="center">如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。</td></tr><tr><td align="center">mod</td><td align="center">计算除法的余数</td><td align="center">5 mod 2</td><td align="center">1</td></tr></tbody></table><h1 id="lxml"><a href="#lxml" class="headerlink" title="lxml"></a>lxml</h1><h2 id="什么是lxml"><a href="#什么是lxml" class="headerlink" title="什么是lxml"></a>什么是lxml</h2><p>lxml 是 一个 HTML/XML 的解析器，主要的功能是如何解析和提取 HTML/XML 数据。lxml 和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的 XPath 语法，来快速的定位特定元素以及节点信息。这里附上<a href="http://lxml.de/index.html">官方文档</a></p><h3 id="先来说一个坑"><a href="#先来说一个坑" class="headerlink" title="先来说一个坑"></a>先来说一个坑</h3><p>我们一般使用lxml库里的etree方法，我第一次使用lxml库的时候被坑的蛮惨，由于我装的是版本较新的lxml库（4.4.1），老版本的导入方法是<code>from lxml import etree</code>，嗯，果不其然报错了。然后就是去找答案，最后发现在比较新的版本里，etree被封装到lxml下的html库里边了，所以可以这样导入：</p><pre class=" language-py"><code class="language-py">from lxml import htmletree = html.etree</code></pre><h2 id="大致流程"><a href="#大致流程" class="headerlink" title="大致流程"></a>大致流程</h2><p>使用requests库和lxml库进行爬虫一般分为以下几个步骤：</p><ol><li><p>导库，设置请求头，利用requests库向网页发送请求，并得到一个response对象</p></li><li><p>利用etree对response对象进行解析，得到一个lxml.etree._Element对象，该对象可以使用xpath语法进行提取信息</p></li><li><p>利用xpath语法对lxml.etree._Element对象进行信息提取</p></li><li><p>将提取的信息进行处理</p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>这里利用requests库和lxml库来爬取豆瓣网经典的电影排名前10的电影信息：</p></li><li><p>导入相关模块：</p><pre class=" language-py"><code class="language-py">import requestsfrom lxml import htmletree = html.etree</code></pre></li><li><p>设置一些参数</p><pre class=" language-py"><code class="language-py">headers = &#123; "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "               "(KHTML, like Gecko) Chrome/80.0.3987.106 Safari/537.36"&#125;#请求头url = "https://movie.douban.com/chart"#目标链接movies = []#用于保存电影信息</code></pre></li><li><p>requests库请求</p><pre class=" language-py"><code class="language-py"># 向指定页数发起请求，并返回resopnse对象response = requests.get(url, headers=headers)</code></pre></li><li><p>etree对response对象进行解析</p><pre class=" language-py"><code class="language-py">html = etree.HTML(response.text)tables = html.xpath("//table")</code></pre></li><li><p>用xpath提取信息并保存</p><pre class=" language-py"><code class="language-py">for table in tables: detail_herf = table.xpath('.//a[@class="nbg"]/@href')[0] name = table.xpath(".//a[@class='nbg']/@title")[0] post_url = table.xpath(".//img/@src")[0] nums = table.xpath(".//span[@class='rating_nums']/text()")[0] movie = &#123;     "href": detail_herf,#详情链接     "name": name,#电影名     "post_url": post_url,#海报链接     "nums": nums#评分 &#125; print(movie) movies.append(movie)</code></pre><p>运行，结果如下图：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594696787504.jpg"><br>最后，再贴一下完整代码：</p><pre class=" language-py"><code class="language-py"># 爬取豆瓣电影经典影片排名import requestsfrom lxml import html</code></pre></li></ol><p>etree = html.etree</p><p>headers = {<br>    “User-Agent”: “Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 “<br>                  “(KHTML, like Gecko) Chrome/80.0.3987.106 Safari/537.36”<br>}<br>url = “<a href="https://movie.douban.com/chart&quot;">https://movie.douban.com/chart&quot;</a><br>movies = []<br>n = 1</p><h1 id="向指定页数发起请求，并返回resopnse对象"><a href="#向指定页数发起请求，并返回resopnse对象" class="headerlink" title="向指定页数发起请求，并返回resopnse对象"></a>向指定页数发起请求，并返回resopnse对象</h1><p>response = requests.get(url, headers=headers)</p><p>html = etree.HTML(response.text)<br>tables = html.xpath(“//table”)</p><p>for table in tables:<br>#这个地方可以借助前面介绍的谷歌浏览器插件来检查匹配是否正确<br>    detail_herf = table.xpath(‘.//a[@class=”nbg”]/@href’)[0]<br>    name = table.xpath(“.//a[@class=’nbg’]/@title”)[0]<br>    post_url = table.xpath(“.//img/@src”)[0]<br>    nums = table.xpath(“.//span[@class=’rating_nums’]/text()”)[0]</p><pre><code>movie = &#123;    &quot;href&quot;: detail_herf,    &quot;name&quot;: name,    &quot;post_url&quot;: post_url,    &quot;nums&quot;: nums&#125;print(movie)movies.append(movie)</code></pre><pre><code># 总结一下嗯，总的看来，写个小爬虫其实是很简单的，xpath语法也很好理解，但仅掌握这些是不够的，无法写出很diao的爬虫，还有蛮多东西要学习！</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python爬虫（3）——cookie与requests库</title>
      <link href="2020/07/05/python-spyder-3/"/>
      <url>2020/07/05/python-spyder-3/</url>
      
        <content type="html"><![CDATA[<h2 id="cookie"><a href="#cookie" class="headerlink" title="cookie"></a>cookie</h2><p>在网站中，http请求是无状态的。也就是说即使第一次和服务器连接后并且登录成功后，第二次请求服务器依然不能知道当前请求是哪个用户。cookie 的出现就是为了解决这个问题，第一次登录后服务器返回一些数据（cookie）给浏览器，然后浏览器保存在本地，当该用户发送第二次请求的时候，就会自动的把上次请求存储的 cookie 数据自动的携带给服务器，服务器通过浏览器携带的数据就能判断当前用户是哪个了。cookie 存储的数据量有限，不同的浏览器有不同的存储大小，但一般不超过 4KB。因此使用 cookie 只能存储一些小量的数据。<br>cookie的一般格式如下：<br><code>Set-Cookie: NAME=VALUE；Expires/Max-age=DATE；Path=PATH；Domain=DOMAIN_NAME； SECURE</code><br>参数的描述如下表：</p><table><thead><tr><th align="center">参数名</th><th align="center">参数描述</th></tr></thead><tbody><tr><td align="center">NAME</td><td align="center">cookie的名字</td></tr><tr><td align="center">VALUE</td><td align="center">cookie的值</td></tr><tr><td align="center">Expires</td><td align="center">cookie的过期时间</td></tr><tr><td align="center">Path</td><td align="center">cookie作用的路径</td></tr><tr><td align="center">Domain</td><td align="center">cookie作用的域名</td></tr><tr><td align="center">SECURE</td><td align="center">是否只在https协议下起作用</td></tr></tbody></table><p>浏览器保存了cookie，后面每次访问时就方便，如果要用代码来向那些需要登录才能访问的（进行更多操作）网站，就需要把cookie信息传给目标服务器，登录说白了也就是要有cookie信息。要获取cookie信息，首先第一想法就是直接去网站上登录，然后按F12，就能找到cookie信息，以人人网（访问演员大鹏主页，需要登录才能访问）为例子：</p><pre class=" language-py"><code class="language-py">from urllib import request,parse#先注释掉cookieheaders=&#123;'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\         AppleWebKit/537.36 (KHTML, like Gecko) \         Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.58',         # 'Cookie': 'ick_login=0ddad9f3-7fa0-4787-b916-5248ea8bb154; anonymid=kca04u42-rmbvi0; depovince=GW; _r01_=1; JSESSIONID=abchsJm52fkNjfdE5xImx; taihe_bi_sdk_uid=15a8a286885699bb6771e6212cc23aa5; taihe_bi_sdk_session=d83833b4a0fb72a6a466205c9329e993; ick=07bea91b-c6d1-4955-ad33-0980db50a672; t=c2a3a8d10309229b0555eb589c8482575; societyguester=c2a3a8d10309229b0555eb589c8482575; id=974715915; xnsid=dd9ddee7; XNESSESSIONID=6c76a75ff812; WebOnLineNotice_974715915=1; ver=7.0; loginfrom=null; jebe_key=060c174f-c2df-4669-9a2c-584b47a0e4a8%7C032338e00e79b21248aaa07a19d3e44d%7C1594009812653%7C1%7C1594009810029; jebe_key=060c174f-c2df-4669-9a2c-584b47a0e4a8%7C032338e00e79b21248aaa07a19d3e44d%7C1594009812653%7C1%7C1594009810031; wp_fold=0; jebecookies=ba0612a2-0ce7-4a4d-95d1-1f71f38ee8ce|||||'    &#125;url="http://www.renren.com/880151247/profile"req = request.Request(url ,headers=headers)resp = request.urlopen(req)with open('result.html','w',encoding = 'utf-8') as f:    f.writelines(resp.read().decode('utf-8'))</code></pre><p>运行代码，会生成一个html文件，我们用浏览器打开它，发现结果如下：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594010413908.jpg"><br>这并不是我们想要的页面，然后将上面的代码中cookie部分取消注释，再运行，打开result.html：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594010616018.jpg"><br>可以看到出现了我们想要的页面！这就很好的体现出cookie的作用。</p><h2 id="http-cookiejar模块"><a href="#http-cookiejar模块" class="headerlink" title="http.cookiejar模块"></a>http.cookiejar模块</h2><p>上面介绍了一种可行的方法，不过我写爬虫不就是为了提高效率嘛，你还每次让我手动去复制粘贴cookie信息？再想想，我们能通过请求获得状态码之类的网站信息，cookie也能用同样的方式获得呀，那不就完事了。这里我们的http.cookiejar模块就闪亮登场了，该模块主要的类有 CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。这四个类的作用分别如下：</p><table><thead><tr><th align="center">类名</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">CookieJar</td><td align="center">管理 HTTP cookie 值、存储 HTTP 请求生成的 cookie、向传出的 HTTP 请求添加 cookie 的对象。整个 cookie 都存储在内存中，对 CookieJar 实例进行垃圾回收后 cookie也将丢失。</td></tr><tr><td align="center">FileCookieJar (filename,delayload=None,policy=None)</td><td align="center">从 CookieJar 派生而来，用创建 FileCookieJar 实例，检索 cookie 信息并将 cookie 存储到文件中。filename 是存储cookie的文件名。delayload 为 True 时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。</td></tr><tr><td align="center">MozillaCookieJar (filename,delayload=None,policy=None)</td><td align="center">从 FileCookieJar 派生而来，创建与 Mozilla 浏览器 cookies.txt 兼容的 FileCookieJar 实例。</td></tr><tr><td align="center">LWPCookieJar (filename,delayload=None,policy=None)</td><td align="center">从 FileCookieJar 派生而来，创建与 libwww-perl 标准的 Set-Cookie3 文件格式兼容的 FileCookieJar 实例。</td></tr></tbody></table><p>再用该模块实现访问人人网大鹏的主页：</p><pre class=" language-py"><code class="language-py">from urllib import request,parsefrom http.cookiejar import CookieJarheaders=&#123;'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\         AppleWebKit/537.36 (KHTML, like Gecko) \         Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.58'    &#125;#创造一个openerdef get_opener():    cookjar=CookieJar()    handler=request.HTTPCookieProcessor(cookjar)    opener=request.build_opener(handler)    return opener#利用创造好的opener发送请求，请求返回的数据中就有cookie信息，他会保存到cookiejar类中def save_cookie(opener):    login_url="http://www.renren.com/PLogin.do"    #账号密码不能泄露啊😂😂😂    data=&#123;"email":"##########",          'password':"##########"        &#125;    res = request.Request(login_url,data= parse.urlencode(data).encode('utf8'),method='POST',headers=headers)    opener.open(res)#访问大鹏主页def get_profile(opener):    url="http://www.renren.com/880151247/profile"    res=request.Request(url,headers=headers)    resp=opener.open(res)    with open('result2.html','w',encoding = 'utf-8') as f:        f.writelines(resp.read().decode('utf-8'))if __name__=='__main__':    opener=get_opener()    save_cookie(opener)    get_profile(opener)</code></pre><p>结果如下：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594037026162.jpg"></p><h2 id="cookie的保存与加载"><a href="#cookie的保存与加载" class="headerlink" title="cookie的保存与加载"></a>cookie的保存与加载</h2><h3 id="保存到本地"><a href="#保存到本地" class="headerlink" title="保存到本地"></a>保存到本地</h3><p>保存 cookie 到本地，可以使用 MozillaCookieJar 的 save 方法，并且需要指定一个文件名。这里我们再用<strong><a href="http://httpbin.org/">http://httpbin.org</a></strong>这个网站来演示，该网站可以让我们自定义该网站的cookie信息，话不多说，先上图；<br>主页是这样的，先点击cookie选项:<br><img src="https://zmlzvt.coding-pages.com//post-images/1594041407808.jpg"><br>跟着步骤来：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594041429670.jpg"><br>设置好后是这样：<img src="https://zmlzvt.coding-pages.com//post-images/1594041443340.jpg"><br>如果想重新设置，可以先清除该网站的cookie数据哦，怎么清除点<a href="https://jingyan.baidu.com/article/09ea3ede592a34c0afde394e.html">它</a></p><pre class=" language-py"><code class="language-py">from urllib import request,parsefrom http.cookiejar import MozillaCookieJarheaders=&#123;'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\         AppleWebKit/537.36 (KHTML, like Gecko) \         Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.58'&#125;url="http://httpbin.org/cookies/set/python/spyder"cookiejar=MozillaCookieJar()handler=request.HTTPCookieProcessor(cookiejar)    opener=request.build_opener(handler)req=request.Request(url,headers=headers)opener.open(req)cookiejar.save('cookie.txt',ignore_discard=True,ignore_expires=True)#由于该网站的cookie不会可能会过期或者被丢弃，所以这里得设置一下参数#ignore_discard 的意思是即使 cookies 将被丢弃也将它保存下来#ignore_expires 的意思是如果 cookies 已经过期也将它保存并且文件已存在时将覆盖with open('cookie.txt','r') as f:    print(f.read())</code></pre><p>可以看到已经生成了cookie.txt文件，里边就存储着cookie信息，结果如下（出现这种格式应该是save方法的问题）：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594041878454.jpg"></p><h3 id="加载cookie信息"><a href="#加载cookie信息" class="headerlink" title="加载cookie信息"></a>加载cookie信息</h3><p>然后再来看看加载cookie信息，跟上边很类似：</p><pre class=" language-py"><code class="language-py">headers=&#123;'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\         AppleWebKit/537.36 (KHTML, like Gecko) \         Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.58'&#125;url="http://httpbin.org/cookies"cookiejar=MozillaCookieJar()cookiejar.load('cookie.txt',ignore_discard=True,ignore_expires=True)handler=request.HTTPCookieProcessor(cookiejar)    opener=request.build_opener(handler)req=request.Request(url,headers=headers)resp=opener.open(req)print(resp.read().decode())</code></pre><p>输出：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594043291139.jpg"></p><h2 id="Requests库"><a href="#Requests库" class="headerlink" title="Requests库"></a>Requests库</h2><p>虽然 Python 的标准库中 urllib 模块已经包含了平常我们使用的大多数功能，但是它的 API使用起来让人感觉不太好，而 Requests 宣传是 “HTTP for Humans”，说明使用更简洁方便。接下来我们就用requests库来实现和urllib同样的功能</p><h3 id="发送get请求"><a href="#发送get请求" class="headerlink" title="发送get请求"></a>发送get请求</h3><p>requests库发送get请求直接使用get方法即可，返回的是一个response对象，如果想添加 headers，可以传入 headers 参数来增加请求头中的 headers 信息。如果要将参数放在 url 中传递，可以利用params 参数</p><pre class=" language-py"><code class="language-py">import requestsurl="https://www.baidu.com"headers=&#123;'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)\         AppleWebKit/537.36 (KHTML, like Gecko) \         Chrome/83.0.4103.116 Safari/537.36 Edg/83.0.478.58'&#125;kw=&#123;'wd':'高考'&#125;# params 是一个字典或者字符串的查询参数，字典自动转换为 url 编码，不需要urlencode()response = requests.get(url, params = kw, headers = headers)# 查看响应内容，response.text 返回的是 Unicode 格式的数据(python 会利用自己猜测的编码方式解码，可能会产生乱码的问题)print(response.text)# 查看响应头部字符编码print(response.encoding)# python 会根据网页内容来得到一个字符编码print(response.apparent_encoding)# 如果 response.text 乱码了，可以先设置response.encoding= response.apparent_encoding# 查看响应内容，response.content 返回的字节流 bytes 数据print(response.content)print(response.content .decode('utf-8'))# 查看完整 url 地址print(response.url)# 查看响应码print(response.status_code)#如果网页返回的是 json，那么可以通过 response.json()转换为 dict 或者 listprint(response.json())</code></pre><h3 id="发送-POST-请求"><a href="#发送-POST-请求" class="headerlink" title="发送 POST 请求"></a>发送 POST 请求</h3><p>最基本的 POST 请求可以使用 post 方法，传入 data 数据（这时候就不要再使用 urlencode 进行编码了，直接传入一个字典进去就可以了）。返回的也是response对象，可以进行的操作同上。比如请求拉勾网的数据的代码：</p><pre class=" language-py"><code class="language-py">import requestsparams = &#123;'px': 'default',          'city': '北京',          'needAddtionalResult': 'false'&#125;data = &#123;'first': 'true',        'pn': 1,        'kd': 'python'&#125;headers = &#123;'Referer': "https://www.lagou.com/jobs/list_python",           'Cookie': 'JSEIONID=ABAAABA9B2BCC46',           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'&#125;url = 'https://www.lagou.com/jobs/list_python#filterBox'response = requests.post(url,                         params=params,                         headers=headers,                         data=data,                         timeout=5)response.encoding = 'UTF-8'print(response.text)</code></pre><h3 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h3><p>使用 requests 添加代理也非常简单，只要在请求的方法中（比如 get 或者 post）传递 proxies参数即可（挂了科学上网软件的最好先关一下，我就因为没关被坑惨了）：</p><pre class=" language-py"><code class="language-py">import requestsurl = 'http://www.httpbin.org/ip'proxy = &#123;    'http': '161.35.110.112:3128'&#125;resp_1 = requests.get(url, timeout=5)print(resp_1.text)resp_2 = requests.get(url, proxies=proxy, timeout=5)print(resp_2.text)</code></pre><p>输出如下：<br><img src="https://zmlzvt.coding-pages.com//post-images/1594091758109.jpg"></p><h3 id="使用requests-session保存cookie"><a href="#使用requests-session保存cookie" class="headerlink" title="使用requests.session保存cookie"></a>使用requests.session保存cookie</h3><p>之前使用 urllib 库，是可以使用 opener 发送多个请求，多个请求之间是可以共享 cookie 的。那么如果使用 requests，也要达到共享 cookie 的目的，那么可以使用 requests 库给我们提供的 session 对象。注意，这里的 session 不是 web 开发中的那个 session，这个地方只是一个会话的对象而已。还是以登录人人网为例，使用 requests 来实现。示例代码如下：</p><pre class=" language-py"><code class="language-py">import requestsurl = "http://www.renren.com/PLogin.do"data = &#123;"email": "970138074@qq.com", 'password': "pythonspider"&#125;headers = &#123;    'UserAgent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.106 Safari/537.36"&#125;# 登录session = requests.session()session.post(url, data=data, headers=headers)# 访问大鹏个人中心resp = session.get('http://www.renren.com/880151247/profile')print(resp.status_code)#输出：200</code></pre><h2 id="总结一下"><a href="#总结一下" class="headerlink" title="总结一下"></a>总结一下</h2><p>个人感觉使用方面确实requests库比起urllib的API接口要舒适许多，所以以后能用requests俺就不用urllib🤣🤣🤣</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python爬虫（2）——urllib库</title>
      <link href="2020/07/02/python-spyder-2/"/>
      <url>2020/07/02/python-spyder-2/</url>
      
        <content type="html"><![CDATA[<h2 id="urllib库简介"><a href="#urllib库简介" class="headerlink" title="urllib库简介"></a>urllib库简介</h2><p>urllib 库是 Python 中一个最基本的网络请求库。可以模拟浏览器的行为，向指定的服务器发 送一个请求，并可以保存服务器返回的数据。</p><h2 id="urlopen函数"><a href="#urlopen函数" class="headerlink" title="urlopen函数"></a>urlopen函数</h2><p>在python3的<code>urllib</code>库中，所有和网络请求相关的方法，都被集成到<code>urllib.request</code>模块下了，该函数的使用格式如下：</p><p><code>urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)</code></p><p>其中：</p><p><code>cafile、capath、cadefault</code> 参数：用于实现可信任的 CA 证书的 HTTP 请求。（基本上很少用）</p><p><code>context</code> 参数：实现 SSL 加密传输。（基本上很少用）</p><p>所以我们一般关注前面几个参数就可以了：</p><ul><li>URL：所请求的URL</li><li>data：请求的data，如果设置了这个参数，请求会变为post请求</li><li>返回值：返回值是一个http.client.HTTPResponse对象，这是一个类文件句柄对象，它主要有以下几种方法：<ol><li>read(size):返回指定字节的数目，默认为全部 </li><li>readline()：返回一行 </li><li>readlines()：用列表返回全部行 </li><li>getcode()：返回状态码，<code>resp.getcode()</code>等价于 <code>resp.status</code> </li><li>getheaders()：返回响应头</li></ol></li></ul><p>下面来看一个例子：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> urllib <span class="token keyword">import</span> requesturl<span class="token operator">=</span><span class="token string">"http://www.baidu.com"</span>resp<span class="token operator">=</span>request<span class="token punctuation">.</span>urlopen<span class="token punctuation">(</span>url<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>resp<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>运行上面的程序，就会打印出百度首页的html源代码，见下图：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593685333801.png"></p><h2 id="request-urlretrieve函数"><a href="#request-urlretrieve函数" class="headerlink" title="request.urlretrieve函数"></a>request.urlretrieve函数</h2><p>这个文件可以很方便的将网页上的文件保存到本地，就比如保存百度首页：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> urllib <span class="token keyword">import</span> requesturl<span class="token operator">=</span><span class="token string">"http://www.baidu.com"</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>url<span class="token punctuation">,</span><span class="token string">'baidu.html'</span><span class="token punctuation">)</span></code></pre><p>运行代码后，就会在代码文件同目录下生成我们所命名的文件，如图：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593685309150.png"><br>同理，也可以用此方法下载图片，以下载一张王者荣耀韩信的图片为例：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> urllib <span class="token keyword">import</span> request<span class="token comment" spellcheck="true">#图片链接只需在网页中鼠标右键选择复制图像链接即可</span>url<span class="token operator">=</span><span class="token string">"https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=1107967462,1249145194&amp;fm=26&amp;gp=0.jpg"</span>request<span class="token punctuation">.</span>urlretrieve<span class="token punctuation">(</span>url<span class="token punctuation">,</span><span class="token string">"韩信.jpg"</span><span class="token punctuation">)</span></code></pre><p>运行后，我们就能看到所下载的图片：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593685278665.png"><br>打开看一下：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593685038035.jpg#pic_centor" alt="Alt"></p><h2 id="parse-urlencode函数和parse-parse-qs函数"><a href="#parse-urlencode函数和parse-parse-qs函数" class="headerlink" title="parse.urlencode函数和parse.parse_qs函数"></a>parse.urlencode函数和parse.parse_qs函数</h2><p>用浏览器发送请求的时候，如果 url 中包含了中文或者其他特殊字符，那么浏览器会自动的给我们进行编码。而如果使用代码发送请求，那么就必须手动的进行编码，这时候就应该使用 urlencode 函数来实现。urlencode 可以把字典数据转换为 URL 编码的数据。示例代码如下：</p><pre class=" language-py"><code class="language-py">from urllib import parsedic1 = &#123;'name': '张三','age' : '18','slogan' : "Hello world"&#125;dic2=parse.urlencode(dic1)print(dic2)#输出如下：name=%E5%BC%A0%E4%B8%89&age=18&slogan=Hello+world</code></pre><p>如果不经过编码，就用urlopen方法的话，会报错：</p><pre class=" language-py"><code class="language-py">from urllib import request,parseresp = request.urlopen('http://www.baidu.com/s?wd=刘德华')#报错如下：UnicodeEncodeError: 'ascii' codec can't encode characters in position 30-32: ordinal not in range(128)</code></pre><p>编码之后就可以了：</p><pre class=" language-py"><code class="language-py">from urllib import request,parseparms = &#123;'wd':'刘德华'&#125;url = 'http://www.baidu.com/s'query_string = parse.urlencode(parms)print(query_string)url = url+'?'+query_stringresp = request.urlopen(url)print(resp.getcode())#输出如下：wd=%E5%88%98%E5%BE%B7%E5%8D%8E200</code></pre><p>parse.parse_qs函数就是将编码后的数据再转变回来的函数：</p><pre class=" language-py"><code class="language-py">from urllib import parsedic1 = &#123;'name': '张三','age' : '18','slogan' : "Hello world"&#125;dic2=parse.urlencode(dic1)print(dic2)print(parse.parse_qs(dic2))#输出如下：name=%E5%BC%A0%E4%B8%89&age=18&slogan=Hello+world&#123;'name': ['张三'], 'age': ['18'], 'slogan': ['Hello world']&#125;#注意返回的数据类型</code></pre><h2 id="parse-urlparse-和-parse-urlsplit-函数"><a href="#parse-urlparse-和-parse-urlsplit-函数" class="headerlink" title="parse.urlparse 和 parse.urlsplit 函数"></a>parse.urlparse 和 parse.urlsplit 函数</h2><p>有时候拿到一个 url，想要对这个 url 中的各个组成部分进行分割，那么这时候就可以使用<br>urlparse 或者是 urlsplit 来进行分割。示例代码如下：</p><pre class=" language-py"><code class="language-py">from urllib import request,parseurl = 'http://www.baidu.com/s?username=zhiliao'result1 = parse.urlsplit(url)result2 = parse.urlparse(url)print(result1)print(result2)</code></pre><p>结果如下图：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593771183471.jpg"><br>从图中可以看出：urlparse 和 urlsplit 基本上是一模一样的。唯一不一样的地方是，urlparse 里面多了一个 params 属性，而 urlsplit 没有这个 params 属性。如果将上面代码的url换一下就能看出区别：</p><pre class=" language-py"><code class="language-py">from urllib import request,parseurl = 'http://www.baidu.com/s;hello?wd=python&username=abc#1'result1 = parse.urlsplit(url)result2 = parse.urlparse(url)print(result1)print(result2)</code></pre><p>结果：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593773675938.jpg"><br>不过params属性很少用到，所以这两种方法基本可以视为相同</p><h2 id="ProxyHandler-处理器（代理设置）"><a href="#ProxyHandler-处理器（代理设置）" class="headerlink" title="ProxyHandler 处理器（代理设置）"></a>ProxyHandler 处理器（代理设置）</h2><p>很多网站会检测某一段时间某个 IP 的访问次数(通过流量统计，系统日志等)，如果访问次数多的不像正常人，它会禁止这个 IP 的访问。所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算 IP 被禁止，依然可以换个 IP 继续爬取。urllib 中通过 ProxyHandler 来设置使用代理服务器，下面代码说明如何使用自定义 opener来使用代理：</p><pre class=" language-py"><code class="language-py">import urlliburl = 'http://httpbin.org/ip'headers = &#123;'User-Agent':"Mozilla/5.0 (Macintosh; U; Mac OS X Mach-O; enUS; rv:2.0a) Gecko/20040614 Firefox/3.0.0 "&#125;req = urllib.request.Request(url,headers=headers)resp = urllib.request.urlopen(req,timeout = 5)print(resp.read().decode())handler = urllib.request.ProxyHandler(&#123;'http':'163.204.240.95:9999'&#125;)opener = urllib.request.build_opener(handler)resp = opener.open(url)print(resp.read().decode())</code></pre><p>运行结果如下图：<br><img src="https://zmlzvt.coding-pages.com//post-images/1593787948833.jpg"><br>可以看出，我们的向目标服务器发送请求的ip地址已经变了，这样就能有效防止目标服务器封我们ip而导致爬虫无法完整运行。</p><blockquote><p>ps:这个网址：<a href="http://httpbin.org/ip%EF%BC%8C%E8%83%BD%E5%B8%AE%E6%88%91%E4%BB%AC%E5%BF%AB%E9%80%9F%E6%9F%A5%E8%AF%A2%E6%88%91%E4%BB%AC%E7%9A%84ip%E5%9C%B0%E5%9D%80%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BF%9D%E5%AD%98%E4%B8%8B%E6%9D%A5%E4%BD%BF%E7%94%A8%E5%93%A6">http://httpbin.org/ip，能帮我们快速查询我们的ip地址，可以保存下来使用哦</a><br>常见的代理获取可以点<a href="http://www.kuaidaili.com/">这里</a>去看看，可以使用免费的代理，当然体验肯定没有付费的代理来得好。</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>小总结一下，用urllib库基本上就可以写一些比较简单的爬虫了，但这里我们想象中的那种强大的爬虫还是有些差距，所以还需要继续学习呀！</p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python爬虫（1）——前置知识</title>
      <link href="2020/07/02/python-spyder-1/"/>
      <url>2020/07/02/python-spyder-1/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是爬虫"><a href="#什么是爬虫" class="headerlink" title="什么是爬虫"></a>什么是爬虫</h3><ul><li>通俗理解：爬虫是一个模拟人类请求网站行为的程序。可以自动请求网页、并数据抓取 下来，然后使用一定的规则提取有价值的数据。</li><li>专业介绍可参考：<a href="https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711?fr=aladdin">百度百科</a><h3 id="什么是-http-和-https-协议"><a href="#什么是-http-和-https-协议" class="headerlink" title="什么是 http 和 https 协议"></a>什么是 http 和 https 协议</h3></li><li>HTTP 协议：全称是 HyperText Transfer Protocol，中文意思是超文本传输协议， 是一种发布和接收 HTML 页面的方法。服务器端口号是 80 端口。</li><li>HTTPS 协议：是 HTTP 协议的加密版本，在 HTTP 下加入了 SSL 层。（数据传输更加安 全）服务器端口号是 443 端口。<h3 id="在浏览器中发送一个-http-请求的过程"><a href="#在浏览器中发送一个-http-请求的过程" class="headerlink" title="在浏览器中发送一个 http 请求的过程"></a>在浏览器中发送一个 http 请求的过程</h3></li></ul><ol><li>当用户在浏览器的地址栏中输入一个 URL 并按回车键之后，浏览器会向 HTTP 服务器 发送 HTTP 请求。HTTP 请求主要分为“Get”和“Post”两种方法。 </li><li>当我们在浏览器输入 URL <a href="http://www.baidu.com/">http://www.baidu.com</a> 的时候，浏览器发送一个 Request 请 求去获取 <a href="http://www.baidu.com/">http://www.baidu.com</a> 的 html 文件，服务器把 Response 文件对象发送回给 浏览器。</li><li>浏览器分析 Response 中的 HTML，发现其中引用了很多其他文件，比如 Images 文件， CSS 文件，JS 文件。 浏览器会自动再次发送 Request 去获取图片，CSS 文件，或者 JS 文件。 </li><li>当所有的文件都下载成功后，网页会根据 HTML 语法结构，完整的显示出来了。<h3 id="url-详解"><a href="#url-详解" class="headerlink" title="url 详解"></a>url 详解</h3>URL 是 Uniform Resource Locator 的简写，统一资源定位符。一个 URL 由以下几部分组成： scheme://host:port/path/?query-string=xxx&amp;query-string=xxx#anchor </li></ol><ul><li><strong>scheme</strong>：代表的是访问的协议，一般为 http 或者 https 以及 ftp 等。 </li><li><strong>host</strong>：主机名，域名，比如 <a href="http://www.baidu.com./">www.baidu.com。</a> </li><li><strong>port</strong>：端口号。当你访问一个网站的时候，浏览器默认使用 80 端口。 </li><li><strong>path</strong>：查找路径。比如：<a href="http://www.jianshu.com/trending/now%EF%BC%8C%E5%90%8E%E9%9D%A2%E7%9A%84">www.jianshu.com/trending/now，后面的</a> trending/now 就是 path。 </li><li><strong>query-string</strong>：查询字符串，比如：<a href="http://www.baidu.com/s?wd=python%EF%BC%8C%E5%90%8E%E9%9D%A2%E7%9A%84">www.baidu.com/s?wd=python，后面的</a> wd=python 就是查询字符串。 </li><li><strong>anchor</strong>：锚点，后台一般不用管，前端用来做页面定位的。(就相当于一个页面中有目 录，目录的跳转就是锚点的作用) 在浏览器中请求一个 url，浏览器会对这个 url 进行一个编码。除英文字母，数字和部分符号 外，其他的全部使用百分号+十六进制码值进行编码。<h3 id="常见请求方法"><a href="#常见请求方法" class="headerlink" title="常见请求方法"></a>常见请求方法</h3>在 Http 协议中，定义了八种请求方法。这里介绍两种常用的请求方法，分别是 get 请求和 post 请求。 </li><li><strong>get 请求</strong>：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响 的时候会使用 get 请求。 </li><li><strong>post 请求</strong>：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候 会使用 post 请求。 以上是在网站开发中常用的两种方法。并且一般情况下都会遵循使用的原则。但是有的 网站和服务器为了做反爬虫机制，也经常会不按常理出牌，有可能一个应该使用 get 方 法的请求就一定要改成 post 请求，这个要视情况而定。</li><li>更多的请求，可以参考：<a href="https://www.runoob.com/http/http-methods.html">菜鸟教程</a><h3 id="常见响应状态码"><a href="#常见响应状态码" class="headerlink" title="常见响应状态码"></a>常见响应状态码</h3></li><li>200：请求正常，服务器正常的返回数据。 </li><li>301：永久重定向。比如在访问 <a href="http://www.jingdong.com/">www.jingdong.com</a> 的时候会重定向到 <a href="http://www.jd.com./">www.jd.com。</a></li><li>302：临时重定向。比如在访问一个需要登录的页面的时候，而此时没有登录，那么就 会重定向到登录页面。 </li><li>400：请求的 url 在服务器上找不到。换句话说就是请求 url 错误。 </li><li>403：服务器拒绝访问，权限不够。 </li><li>500：服务器内部错误。可能是服务器出现 bug 了。<h3 id="请求头常见参数"><a href="#请求头常见参数" class="headerlink" title="请求头常见参数"></a>请求头常见参数</h3>在 http 协议中，向服务器发送一个请求，数据分为三部分，第一个是把数据放在 url 中，第 二个是把数据放在 body 中（在 post 请求中），第三个就是把数据放在 head 中。 这里介绍在网络爬虫中经常会用到的一些请求头参数： </li><li><strong>User-Agent</strong>：浏览器名称。这个在网络爬虫中经常会被使用到。请求一个网页的时候， 服务器通过这个参数就可以知道这个请求是由哪种浏览器发送的。如果我们是通过爬虫 发送请求，那么我们的 User-Agent 就是 Python，这对于那些有反爬虫机制的网站来说， 可以轻易的判断你这个请求是爬虫。因此我们要经常设置这个值为一些浏览器的值，来 伪装我们的爬虫。 </li><li><strong>Referer</strong>：表明当前这个请求是从哪个 url 过来的。这个一般也可以用来做反爬虫技术。 如果不是从指定页面过来的，那么就不做相关的响应。 </li><li><strong>Cookie</strong>：http 协议是无状态的。也就是同一个人发送了两次请求，服务器没有能力知道 这两个请求是否来自同一个人。因此这时候就用 cookie 来做标识。一般如果想要做登 录后才能访问的网站，那么就需要发送 cookie 信息了。</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习笔记——numpy（2）</title>
      <link href="2020/06/16/numpy-2/"/>
      <url>2020/06/16/numpy-2/</url>
      
        <content type="html"><![CDATA[<p>继续来学习numpy：</p><h2 id="一、索引和切片"><a href="#一、索引和切片" class="headerlink" title="一、索引和切片"></a>一、索引和切片</h2><p>前面学了用numpy来创建各种各样的数组，这些数组都是<code>ndarray</code>对象，都支持索引和切片操作。<code>ndarray</code>对象的索引与python中的<code>list</code>基本一致：</p><pre class=" language-py"><code class="language-py">>>> import numpy as np>>> a=np.arange(1,6,1)>>> aarray([1, 2, 3, 4, 5])>>> a[0]#也是从0开始索引1>>> a[4]5#再看二维数组索引>>> aa=np.array([[1,2,3],[4,5,6],[7,8,9]])>>> aaarray([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]]) >>> aa[0,0]1>>> aa[2,2]9</code></pre><p>更高维度数组元素的索引和上述方式大同小异，再来看切片，先用和列表一样的方法来试试：</p><pre class=" language-py"><code class="language-py">>>> b=np.arange(0,10,1)>>> barray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>>> b[4:]array([4, 5, 6, 7, 8, 9])>>> b[0:4]array([0, 1, 2, 3])>>> b[5:-1]array([5, 6, 7, 8])>>> b[0:5:2]array([0, 2, 4])</code></pre><p>可以发现，这种方法是完全可行的，还可以利用python内置函数<code>slice</code>进行切片：</p><pre class=" language-py"><code class="language-py">>>> b[slice(1,8,2)]#取出b中从索引1开始到索引8的元素，间隔为2array([1, 3, 5, 7])</code></pre><p>看完了一维数组，再来看看二维数组元素的切片,二维数组里边我们经常有取一整列或者一整行的需求，可以使用<code>...</code>和<code>:</code>来进行切片，具体操作如下：</p><pre class=" language-py"><code class="language-py">>>> bb=np.array([[1,2,3],[4,5,6],[7,8,9]])>>> bbarray([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])>>> bb[...,0]#取数组bb里第一列元素array([1, 4, 7])>>> bb[:,0]#跟bb[...,0]作用相同array([1, 4, 7])>>> bb[1,...]#取数组bb里第一行元素array([4, 5, 6])>>> bb[1,:]#跟bb[1,...]作用相同array([4, 5, 6])#还可以取几列或者几行元素>>> bb[:,0:2]#取第0列和第1列全部元素array([[1, 2],       [4, 5],       [7, 8]])>>> bb[0:2,:]#取第0行和第1行的全部元素array([[1, 2, 3],       [4, 5, 6]])>>> bb[0:2,1:3]#取第0行和第1行上第1列和第2列的元素array([[2, 3],       [5, 6]])>>> bb[:]#取全部元素array([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])</code></pre><h2 id="二、数组变形"><a href="#二、数组变形" class="headerlink" title="二、数组变形"></a>二、数组变形</h2><p>numpy里面的数组，还可以进行变形操作，主要有以下几种操作：</p><h3 id="1-改变数组形状"><a href="#1-改变数组形状" class="headerlink" title="1.改变数组形状"></a>1.改变数组形状</h3><p>在numpy中，封装了reshape函数来对数组进行变形操作，它一共接收三个参数：<code>numpy.reshape(arr, newshape, order=&#39;C&#39;)</code>，各个参数的描述如下：<br>|参数|描述|<br>|:-:|:-:|<br>|arr|需要修改形状的数组|<br>|reshape|修改后数组的形状，传入的应该是整数或者整数数组,而且总的元素数量应该与arr一致|<br>|order|’C’ – 按行，’F’ – 按列，’A’ – 原顺序，’k’ – 元素在内存中的出现顺序|<br>来看一下例子：</p><pre class=" language-py"><code class="language-py">>>> a1=np.arange(1,10,1)>>> a1array([1, 2, 3, 4, 5, 6, 7, 8, 9])>>> a2=np.reshape(a1,[3,3])>>> a2array([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])>>> a3=np.reshape(a1,[3,3],order='F')>>> a3array([[1, 4, 7],       [2, 5, 8],       [3, 6, 9]])>>> a4=np.reshape(a1,[3,3],order='A')>>> a4array([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])</code></pre><p>也可以使用a1.reshape()来进行数组变形，跟上面的效果一样</p><pre class=" language-py"><code class="language-py">>>> a6=a1.reshape([3,3])>>> a6array([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])</code></pre><h3 id="2-转置"><a href="#2-转置" class="headerlink" title="2.转置"></a>2.转置</h3><p>矩阵转置有两种方法：</p><ul><li><strong>transpose函数</strong></li><li><strong>ndarray.T</strong><br>二者基本上等价，来看个例子：<pre class=" language-py"><code class="language-py">>>> b1=np.arange(1,9,1).reshape([2,4])>>> b1array([[1, 2, 3, 4],     [5, 6, 7, 8]])>>> b2=np.transpose(b1)>>> b2array([[1, 5],     [2, 6],     [3, 7],     [4, 8]])>>> b3=b1.T>>> b3array([[1, 5],     [2, 6],     [3, 7],     [4, 8]])</code></pre><h3 id="3-广播（broadcost）"><a href="#3-广播（broadcost）" class="headerlink" title="3.广播（broadcost）"></a>3.广播（broadcost）</h3>我们知道，矩阵运算通常对矩阵的维度有要求，所以numpy里边用广播机制来使矩阵满足维度要求。当运算中的 2 个数组的形状不同时，numpy 将自动触发广播机制。如：<pre class=" language-py"><code class="language-py">>>> import numpy as np>>> a=np.array([[0,0,0],[1,1,1],[2,2,2],[3,3,3]])>>> aarray([[0, 0, 0],     [1, 1, 1],     [2, 2, 2],     [3, 3, 3]])>>> b=np.array([1,2,3])>>> a+barray([[1, 2, 3],     [2, 3, 4],     [3, 4, 5],     [4, 5, 6]]</code></pre>广播机制内容较多，这里只简单介绍，详细学习可参考<a href="https://www.cnblogs.com/jiaxin359/p/9021726.html">这篇博客</a><h2 id="三、通用函数"><a href="#三、通用函数" class="headerlink" title="三、通用函数"></a>三、通用函数</h2>通用函数，也可以称为ufunc，是一种在ndarray数据中进行<strong>逐元素操作</strong>的函数。某些简单函数接受一个或多个标量数值，并产生一个或多个标量数值，而通用函数就是对这些简单函数的向量化封装<h3 id="3-1一元通用函数"><a href="#3-1一元通用函数" class="headerlink" title="3.1一元通用函数"></a>3.1一元通用函数</h3>有很多ufunc是简单的逐元素转换，比如<code>sqrt</code>或者<code>exp</code>函数：<pre class=" language-py"><code class="language-py">>>> arr=np.arange(10)>>> arrarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>>> np.sqrt(arr)array([0.        , 1.        , 1.41421356, 1.73205081, 2.        ,     2.23606798, 2.44948974, 2.64575131, 2.82842712, 3.        ])>>> np.exp(arr)array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,     5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,     2.98095799e+03, 8.10308393e+03])</code></pre>这类函数我们把它称为一元通用函数，在这里列举一些常见的一元通用函数：<table><thead><tr><th align="center">函数名</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">abs、fabs</td><td align="center">逐元素的计算整数、浮点数或者复数的绝对值</td></tr><tr><td align="center">sqrt</td><td align="center">计算每个元素的平方根（与$arr^{0.5}$等价）</td></tr><tr><td align="center">square</td><td align="center">计算每个元素的平方（与$arr^2$等价）</td></tr><tr><td align="center">exp</td><td align="center">计算每个元素的以自然底数为底的指数值$e^2$</td></tr><tr><td align="center">log、log10、log2、log1p</td><td align="center">分别对应：自然对数（e为底）、对数10为底、对数2为底、$log(1+x)$</td></tr><tr><td align="center">sign</td><td align="center">计算每个元素的符号值：1（表示正数）、0表示数字0、-1表示复数</td></tr><tr><td align="center">ceil</td><td align="center">计算每个元素的向上取整（大于等于给定数字的最小整数）</td></tr><tr><td align="center">floor</td><td align="center">计算每个元素的向下取整（小于等于给定数字的最大整数）</td></tr><tr><td align="center">rint</td><td align="center">将每个元素保留整数位，并保持dtype</td></tr><tr><td align="center">modf</td><td align="center">将每个元素的整数位和小数位分开，并按数组形式返回</td></tr><tr><td align="center">isnan</td><td align="center">判断各个元素是不是NAN（不是一个数值）并返回布尔值数组</td></tr><tr><td align="center">isfinite、isinf</td><td align="center">判断数组中各元素是否有限、是否无限，并返回布尔值数组</td></tr><tr><td align="center">cos、cosh、sin、sinh、tan、tanh</td><td align="center">常规的双曲三角函数</td></tr><tr><td align="center">arccos、arccosh、arcsin、arcsinh、arctan、arctanh</td><td align="center">反三角函数</td></tr><tr><td align="center">logical_not</td><td align="center">对数组的元素按位取反（与-arr等价）</td></tr></tbody></table><h3 id="3-2二元通用函数"><a href="#3-2二元通用函数" class="headerlink" title="3.2二元通用函数"></a>3.2二元通用函数</h3>还有一些通用函数，比如add或者maximum则会接收两个数组并返回一个数组作为结果，因此成为二元通用函数，在这列举一些：<table><thead><tr><th align="center">函数名</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">add</td><td align="center">将数组的对应元素相加</td></tr><tr><td align="center">subtract</td><td align="center">在第二个数组中，将第一个数组中包含的元素去除</td></tr><tr><td align="center">multiply</td><td align="center">将数组的对应元素相乘</td></tr><tr><td align="center">devide、floor_devide</td><td align="center">除或整除（放弃余数）</td></tr><tr><td align="center">power</td><td align="center">将第二个数组的元素作为第一个数组对应元素的幂次方</td></tr><tr><td align="center">maximum、fmax</td><td align="center">逐个元素计算最大值，fmax会忽略NaN</td></tr><tr><td align="center">minimum、fmin</td><td align="center">逐个元素计算最小值，fmin会忽略NaN</td></tr><tr><td align="center">mod</td><td align="center">按元素的求模计算（即求除法的余数）</td></tr><tr><td align="center">copysign</td><td align="center">将第一个数组的符号值改为第二个数组的符号值</td></tr><tr><td align="center">greater、greater_equal、less、less_equal、equal、not_equal</td><td align="center">进行逐个元素的比较，返回布尔值数组（分别与数学操作符<code>&gt;,&gt;=,&lt;,&lt;=,==,!=</code>对应效果一致）</td></tr><tr><td align="center">logical_and、logical_or、logical_xor</td><td align="center">进行逐个元素的逻辑操作，与逻辑操作`&amp;,</td></tr></tbody></table></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> numpy </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python学习笔记——numpy（1）</title>
      <link href="2020/06/14/numpy-1/"/>
      <url>2020/06/14/numpy-1/</url>
      
        <content type="html"><![CDATA[<p>学过python的人应该都接触过<a href="https://baike.baidu.com/item/numpy/5678437?fr=aladdin"><strong>numpy</strong></a>这个有名的第三方库，它是一个支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。</p><h1 id="1-安装numpy"><a href="#1-安装numpy" class="headerlink" title="1.安装numpy"></a>1.安装numpy</h1><p>安装方法有两种：</p><ul><li>pip安装：直接在命令行里键入<code>pip install numpy</code>即可，如果遇到安装速度过慢的情况，可以考虑到清华镜像源下载安装。</li><li>conda安装，在anaconda powershell里输入<code>conda install numpy</code>,速度慢也可以换源。<br>我这里已经安装好了，显示如下：<br><img src="https://zmlzvt.coding-pages.com//post-images/1592147524509.jpg"><br>打开idle验证一下安装是否成功：<pre class=" language-py"><code class="language-py">>>> import numpy as np>>> np.__version__#打印版本信息'1.16.5'>>> np.ones(5)#生成值全为1的数组array([1., 1., 1., 1., 1.])</code></pre>正确生成了数组，说明numpy安装成功<h1 id="2-基础操作"><a href="#2-基础操作" class="headerlink" title="2. 基础操作"></a>2. 基础操作</h1><h2 id="2-1通过维度创建数组"><a href="#2-1通过维度创建数组" class="headerlink" title="2.1通过维度创建数组"></a>2.1通过维度创建数组</h2>先看由指定维度来创建数组的方法：<pre class=" language-py"><code class="language-py">>>> a=np.array([1,2,3])#创建一维数组>>> aarray([1, 2, 3]）>>> b=np.array([[1,2,3],[4,5,6]])#创建二维数组，高维度同理>>> barray([[1, 2, 3],     [4, 5, 6]])>>> np.zeros([3,3])#生成指定维度的值全为0的数组array([[0., 0., 0.],     [0., 0., 0.],     [0., 0., 0.]])>>> np.ones([3,3])##生成指定维度的值全为1的数组array([[1., 1., 1.],     [1., 1., 1.],     [1., 1., 1.]])>>> np.empty([3,3])#生成指定维度的值为随机数的数组array([[1., 1., 1.],     [1., 1., 1.],     [1., 1., 1.]])>>> np.empty_like(b)#生成和b数组同维度的值为随机数的数组array([[0, 0, 0],     [0, 0, 0]])</code></pre>在实际操作中，通过生成a，b数组的方式来创建高维度数组时会显得很麻烦，所以一般使用<code>np.ones</code>或<code>np.zeros</code>来生成指定维度的数组，然后对其中的元素进行修改。<h2 id="2-2通过数值范围创建数组"><a href="#2-2通过数值范围创建数组" class="headerlink" title="2.2通过数值范围创建数组"></a>2.2通过数值范围创建数组</h2><h3 id="2-2-1等差数列"><a href="#2-2-1等差数列" class="headerlink" title="2.2.1等差数列"></a>2.2.1等差数列</h3>有时候，我们需要通过数值范围来创建数组，比如现在需要一个从1到100的公差为2的等差数列，通过前面的方法也可以创建（先生成指定维度的0数组，再利用循环进行赋值），这样会显得比较繁琐，可以使用<code>arange</code>函数：<pre class=" language-py"><code class="language-py">#以生成1到20的公差为2的等差数列为例>>> np.arange(1,20,2)#array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])>>> np.arange(start=1,stop=20,step=2)#详细写法array([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])>>> np.arange(1,20,1)array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,     18, 19])</code></pre><code>arange</code>函数中需要三个参数，依次是起始点、终止点和步长，可以看出：即使步长设置为1，输出的数组也是不包括终止点的，所以<code>arange</code>函数的输入可以看做一个左闭右开区间，如果想要包括终止点，可以让终止点增大一个步长。</li></ul><p>除了<code>arange</code>函数，还有一个<code>linspace</code>函数也可以生成等差数列：</p><pre class=" language-py"><code class="language-py">#以生成1到10的公差为1的等差数列为例>>> np.linspace(1,10,10)array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])>>> np.linspace(start=1,stop=10,num=10)#详细写法array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])>>> np.linspace(1,10,10,endpoint=False)array([1. , 1.9, 2.8, 3.7, 4.6, 5.5, 6.4, 7.3, 8.2, 9.1])</code></pre><p><code>linspace</code>函数中需要至少三个参数，依次为起始点、终止点和要生成的等间距点的数量，还有一个可选参数<code>endpoint</code>，它的默认值为<code>True</code>，表示默认包括终止点，此时可将输入理解为一个闭区间，将其设置为<code>False</code>后，输入又变成了一个左闭右开区间。</p><h3 id="2-2-2等比数列"><a href="#2-2-2等比数列" class="headerlink" title="2.2.2等比数列"></a>2.2.2等比数列</h3><p><code>numpy</code>同样有函数来创建满足需要的等比数列，函数名字叫<code>logspace</code>，该函数有<code>start</code>、<code>stop</code>、<code>num</code>、<code>endpoint</code>、<code>base</code>五个参数，<br>|参数   |描述|<br>|:-: | :-: |<br>|start       |表示所生成数列的起始值为${base}^{start}$            |<br>|stop        |表示所生成数列的终止值为$base^{stop}$          |<br>|num        |要生成的等步长的样本数量，默认为50|<br>|endpoint|该值为 True 时，数列中中包含$base^{stop}$ 值，反之不包含，默认是True。|<br>|base|    对数 log 的底数，默认值为10|<br>实例如下：</p><pre class=" language-py"><code class="language-py">#生成1到10000的等差数列>>> np.logspace(0,4,5)array([1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04])>>> np.logspace(0,4,5,endpoint=False)array([1.00000000e+00, 6.30957344e+00, 3.98107171e+01, 2.51188643e+02,       1.58489319e+03]#生成1到128的等差数列>>> np.logspace(0,7,8,base=2)array([  1.,   2.,   4.,   8.,  16.,  32.,  64., 128.])</code></pre><h2 id="2-3数据类型"><a href="#2-3数据类型" class="headerlink" title="2.3数据类型"></a>2.3数据类型</h2><p>在上述的各种函数中，其实都还有一个参数，名字为<code>dtype</code>，它可以用来指定数组中元素的类型，举一个例子：</p><pre class=" language-py"><code class="language-py">>>> c=np.array([1,2,3,4])>>> carray([1, 2, 3, 4])>>> c.dtypedtype('int32')>>> d=np.array([1,2,3,4],dtype='float32')>>> darray([1., 2., 3., 4.], dtype=float32)</code></pre><p>这里再附上numpy里的数据类型表（转载于<a href="https://www.runoob.com/numpy/numpy-dtype.html">菜鸟教程</a>）：<br>|名称|    描述|<br>|:-:|:-:|<br>|bool_    |布尔型数据类型（True 或者 False）<br>|int_    |默认的整数类型（类似于 C 语言中的 long，int32 或 int64）<br>|intc    |与 C 的 int 类型一样，一般是 int32 或 int 64<br>|intp    |用于索引的整数类型（类似于 C 的 ssize_t，一般情况下仍然是 int32 或 int64）<br>|int8    |字节（-128 to 127）<br>|int16    |整数（-32768 to 32767）<br>|int32    |整数（-2147483648 to 2147483647）<br>|int64    |整数（-9223372036854775808 to 9223372036854775807）<br>|uint8    |无符号整数（0 to 255）<br>|uint16    |无符号整数（0 to 65535）<br>|uint32    |无符号整数（0 to 4294967295）<br>|uint64    |无符号整数（0 to 18446744073709551615）<br>|float_    |float64 类型的简写<br>|float16    |半精度浮点数，包括：1 个符号位，5 个指数位，10 个尾数位<br>|float32    |单精度浮点数，包括：1 个符号位，8 个指数位，23 个尾数位<br>|float64    |双精度浮点数，包括：1 个符号位，11 个指数位，52 个尾数位<br>|complex_    |complex128 类型的简写，即 128 位复数<br>|complex64    |复数，表示双 32 位浮点数（实数部分和虚数部分）<br>|complex128    |复数，表示双 64 位浮点数（实数部分和虚数部分）</p>]]></content>
      
      
      
        <tags>
            
            <tag> numpy </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
